
## üß† KNN para Classifica√ß√£o

Nesta etapa, conhecemos o algoritmo **KNN (k-Nearest Neighbors)** em sua aplica√ß√£o para **classifica√ß√£o**. O modelo se baseia em uma ideia simples: ao receber um novo dado, ele verifica quais s√£o seus **k vizinhos mais pr√≥ximos** (com base na dist√¢ncia euclidiana, por exemplo) e o classifica de acordo com a **classe mais frequente** entre esses vizinhos.

### üì¶ Como funciona?

* O modelo **n√£o faz c√°lculos complexos internamente** nem cria f√≥rmulas ‚Äî ele apenas **armazena os dados de treino**.
* Quando um novo dado √© apresentado, o algoritmo:

  1. Calcula a dist√¢ncia entre esse novo dado e todos os dados j√° conhecidos.
  2. Seleciona os `k` mais pr√≥ximos.
  3. Verifica a classe predominante entre eles.
  4. Atribui essa classe ao novo dado.

### ‚öôÔ∏è Implementa√ß√£o com `sklearn`

```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
```

### üìä Usando um conjunto de dados (ex: frutas)

```python
import pandas as pd
from sklearn.model_selection import train_test_split

# Carregando dados
data = pd.read_table('fruit_data_with_colors.txt')
X = data[['mass', 'height', 'width', 'color_score']]
y = data['fruit_label']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Treinando o modelo
knn.fit(X_train, y_train)

# Avaliando o modelo
print(knn.score(X_test, y_test))
```

### üìâ Avalia√ß√£o

O desempenho inicial pode n√£o ser muito alto (ex: 60%) e isso pode ser causado por **diferen√ßas de escala entre os atributos**, como massa e cor. Como o KNN depende fortemente da **dist√¢ncia**, atributos com valores em escalas maiores podem dominar os c√°lculos. Solu√ß√£o? **Normalizar ou padronizar os dados**, o que ser√° abordado nos pr√≥ximos passos.

