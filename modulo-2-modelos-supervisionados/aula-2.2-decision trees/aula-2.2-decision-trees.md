# üå≥ Introdu√ß√£o √†s √Årvores de Decis√£o
---

## üß† Por que aprender √Årvores de Decis√£o?

As √°rvores de decis√£o s√£o uma excelente porta de entrada para o mundo do machine learning por dois motivos principais:

* **Facilidade de interpreta√ß√£o:** √© poss√≠vel entender claramente o que est√° motivando uma decis√£o do modelo.
* **Processo l√≥gico:** seguem um racioc√≠nio pr√≥ximo ao humano, facilitando a an√°lise e valida√ß√£o por especialistas da √°rea.

---

## üìä O que voc√™ vai ver aqui?

Para facilitar o entendimento, utilizamos um exemplo bem pr√°tico: a classifica√ß√£o de **peixes** com base em caracter√≠sticas como **brilho** e **tamanho**. Nosso objetivo √© prever se um peixe √© **Salm√£o** ou **Sea Bass (Badejo)**.

Durante essa jornada, voc√™ vai acompanhar:

* Como √© constru√≠da uma √°rvore de decis√£o a partir de um conjunto de dados.
* Como definimos as regras de decis√£o com base nos atributos.
* Como avaliamos a ‚Äúqualidade‚Äù das divis√µes usando o **√≠ndice de Gini** ‚Äî uma medida de impureza bastante comum em modelos de classifica√ß√£o.

---

## üß™ Como funciona na pr√°tica?

Come√ßamos com uma pequena base de dados contendo:

* `Brilho`: medida de frescor ou qualidade do peixe.
* `Tamanho`: comprimento do peixe.
* `Classifica√ß√£o`: r√≥tulo com a esp√©cie (Salm√£o ou Sea Bass).

A partir disso, constru√≠mos uma √°rvore com regras como:

```text
Se Tamanho > 25 ‚û°Ô∏è Salm√£o  
Se Tamanho <= 25 e Brilho > 0.9 ‚û°Ô∏è Salm√£o  
Se Tamanho <= 25 e Brilho <= 0.9 ‚û°Ô∏è Sea Bass  
```

Essas regras s√£o criadas de forma autom√°tica, com base na melhor divis√£o dos dados segundo o √≠ndice de Gini. Quanto menor o valor do Gini, mais "pura" √© a divis√£o, ou seja, melhor √© a separa√ß√£o entre as classes.

---

## üßÆ Um pouco de matem√°tica (sem exagero!)

O √≠ndice de Gini √© calculado assim:

```
Gini = 1 - ‚àë (p_i¬≤)
```

Onde `p_i` √© a propor√ß√£o de elementos da classe `i` em um grupo. Testamos v√°rias divis√µes nos dados e escolhemos aquela que gera o menor √≠ndice Gini ‚Äî isto √©, a divis√£o mais ‚Äúlimpa‚Äù.

---

## üîÅ E depois?

Esse processo √© repetido em cada grupo de dados criado at√© que n√£o haja mais impurezas ‚Äî ou seja, at√© que cada ramo da √°rvore leve a uma √∫nica classe.

Assim, obtemos uma **√°rvore de decis√£o completa**, pronta para classificar novos dados com base nas regras extra√≠das do conjunto original.

---
